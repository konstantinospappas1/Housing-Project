import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import randint, uniform

# === 1. Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ===
df = pd.read_csv('Housing.csv', encoding='utf-8')
df = df.dropna()
print(f"Î”ÎµÎ´Î¿Î¼Î­Î½Î±: {len(df)} Î³ÏÎ±Î¼Î¼Î­Ï‚")

# === 2. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ X ÎºÎ±Î¹ y ===
X = df.drop('price', axis=1)
y = df['price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# === 3. OneHot Encoding ===
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
encoder = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)],
    remainder='passthrough'
)

X_train = encoder.fit_transform(X_train)
X_test = encoder.transform(X_test)
print(f"Features Î¼ÎµÏ„Î¬ Ï„Î¿ encoding: {X_train.shape[1]}")

# === 4. ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î²Î±ÏƒÎ¹ÎºÎ¿Ï Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ===
xgb = XGBRegressor(random_state=42, n_jobs=-1)

# === 5. Randomized Search (100 ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼Î¿Î¯) ===
param_dist = {
    'n_estimators': randint(5, 30),     
    'max_depth': randint(2, 8),            # 2â€“7
    'learning_rate': uniform(0.01, 0.4),   # 0.01â€“0.41
    'subsample': uniform(0.4, 0.6),        # 0.4â€“1.0
    'colsample_bytree': uniform(0.5, 0.9), # 0.5â€“0.9
    'min_child_weight': randint(1, 10)     # 1â€“9
}

random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=200,           # <= 50 Î´Î¿ÎºÎ¹Î¼Î­Ï‚, Ï€Î¿Î»Ï Î³ÏÎ®Î³Î¿ÏÎ¿
    scoring='r2',
    cv=3,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

print("\nÎÎµÎºÎ¹Î½Î¬ Ï„Î¿ Randomized Search...")
random_search.fit(X_train, y_train)
print("\nâœ… Î¤Î­Î»Î¿Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚")

# === 6. Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ===
print("\n" + "="*60)
print("ğŸ“Š ÎšÎ‘Î›Î¥Î¤Î•Î¡Î•Î£ Î Î‘Î¡Î‘ÎœÎ•Î¤Î¡ÎŸÎ™ Î‘Î ÎŸ RANDOMIZED SEARCH")
print("="*60)
print(random_search.best_params_)
print(f"ÎœÎ­ÏƒÎ¿ RÂ² Î±Ï€ÏŒ CV: {random_search.best_score_:.4f}")

# === 7. Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î¹Ï‚ ÎºÎ±Î»ÏÏ„ÎµÏÎµÏ‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ ===
best_model = random_search.best_estimator_
best_model.fit(X_train, y_train)

# === 8. Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ===
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)

print("\n" + "="*60)
print("ğŸ“ˆ Î¤Î•Î›Î™ÎšÎ‘ Î‘Î ÎŸÎ¤Î•Î›Î•Î£ÎœÎ‘Î¤Î‘")
print("="*60)
print(f"Train MAE: {train_mae:,.2f}")
print(f"Test MAE:  {test_mae:,.2f}")
print(f"Train RÂ²:  {train_r2:.4f}")
print(f"Test RÂ²:   {test_r2:.4f}")
print("="*60)

# === 9. Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½ ===
df_compare = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred_test
})
print("\nÎ Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÏ‰Î½:")
print(df_compare.head(10))
